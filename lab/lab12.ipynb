{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: TensorFlow & Logistic/Softmax Regression\n",
    "\n",
    "In this lab, we are going to briefly go through a widely used deep learning framework -- Tensorflow and implement\n",
    "logistic regression with TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install TensorFlow**\n",
    "\n",
    "- Run the following cell. If running into issues\n",
    "    - For Mac users, please refer to https://www.tensorflow.org/install/install_mac (install with Anaconda)\n",
    "    - For Windows users, please refer to https://www.tensorflow.org/install/install_windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.4.0 from https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl\n",
      "  Using cached https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl\n",
      "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.0)\n",
      "  Using cached tensorflow_tensorboard-0.4.0rc3-py3-none-any.whl\n",
      "Collecting six>=1.10.0 (from tensorflow==1.4.0)\n",
      "  Using cached six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.3.0 (from tensorflow==1.4.0)\n",
      "  Using cached protobuf-3.5.0.post1-py2.py3-none-any.whl\n",
      "Collecting wheel>=0.26 (from tensorflow==1.4.0)\n",
      "  Using cached wheel-0.30.0-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.12.1 (from tensorflow==1.4.0)\n",
      "  Using cached numpy-1.13.3-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting enum34>=1.1.6 (from tensorflow==1.4.0)\n",
      "  Using cached enum34-1.1.6-py3-none-any.whl\n",
      "Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\n",
      "  Using cached bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\n",
      "Collecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\n",
      "  Using cached Werkzeug-0.12.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0)\n",
      "Collecting setuptools (from protobuf>=3.3.0->tensorflow==1.4.0)\n",
      "  Using cached setuptools-38.2.3-py2.py3-none-any.whl\n",
      "Installing collected packages: six, setuptools, protobuf, html5lib, bleach, numpy, werkzeug, markdown, wheel, tensorflow-tensorboard, enum34, tensorflow\n",
      "Successfully installed bleach-1.5.0 enum34-1.1.6 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.3 protobuf-3.5.0.post1 setuptools-38.2.3 six-1.11.0 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0rc3 werkzeug-0.12.2 wheel-0.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.4.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Test your installtion \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from IPython.display import display, Latex, Markdown\n",
    "from client.api.notebook import Notebook\n",
    "# ok = Notebook('lab12.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's lab, we're going to use logistic regression to classify handwritten digits. You'll learn about logistic / softmax regression and TensorFlow, a popular machine learning library developed by Google.\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is a library typically used to train deep neural networks (DNNs).  DNN learning is just like linear regression or classification, except that we search over a more complicated class of functions, not just linear ones.  DNNs have been popularized by their success in many fields, such as in spam detection, speech recognition, or even in art, such as [Neural Style](https://github.com/anishathalye/neural-style).  They are a building block in many successful applications of machine learning in recent years.\n",
    "\n",
    "Protip: This lab is taken straight from the [TensorFlow tutorials](https://www.tensorflow.org/get_started/mnist/beginners) so if you get stuck, go ahead and reference that page.\n",
    "\n",
    "## Digitize it\n",
    "\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset is comprised of 60,000 handwritten digits from 0-9 (10 total types).  The data are *greyscale pixels* from scans of handwriting.\n",
    "\n",
    "Let's load in and take a peek at the data. The next cell will download and load the data into a variable called `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Statistics\n",
    "mnist.train.images.shape, mnist.validation.images.shape, mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[0,1],[2,1]]\n",
    "b=[[1,1],[2,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1],\n",
       "       [2, 1, 2, 2]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.column_stack((a,b))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training example is originally a 28x28 image:\n",
    "\n",
    "![](https://www.tensorflow.org/images/MNIST-Matrix.png)\n",
    "\n",
    "To make it easier for machine learning, the images are flattened out into length-784 vectors.\n",
    "\n",
    "Here's a function to reshape the vector back into a 28x28 image and a function to display one / multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def example_to_image(example):\n",
    "    '''Takes in a length-784 training example and returns a (28, 28) image.'''\n",
    "    return example.reshape((28, 28))\n",
    "\n",
    "def show_images(images, ncols=2, figsize=(10, 7), **kwargs):\n",
    "    \"\"\"\n",
    "    Shows one or more images.\n",
    "    \n",
    "    images: Image or list of images.\n",
    "    \"\"\"\n",
    "    def show_image(image, axis=plt):\n",
    "        plt.imshow(image, cmap='gray', **kwargs)\n",
    "        \n",
    "    if not (isinstance(images, list) or isinstance(images, tuple)):\n",
    "        images = [images]\n",
    "    \n",
    "    nrows = math.ceil(len(images) / ncols)\n",
    "    ncols = min(len(images), ncols)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, image in enumerate(images):\n",
    "        axis = plt.subplot2grid(\n",
    "            (nrows, ncols),\n",
    "            (i // ncols,  i % ncols),\n",
    "        )\n",
    "        axis.tick_params(bottom='off', left='off', top='off', right='off',\n",
    "                         labelleft='off', labelbottom='off')\n",
    "        axis.grid(False)\n",
    "        show_image(image, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "# Question 1\n",
    "\n",
    "Use the provided `example_to_image` and `show_images` function to visualize the training examples given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.12156864,  0.51764709,\n",
       "        0.99607849,  0.99215692,  0.99607849,  0.83529419,  0.32156864,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.08235294,\n",
       "        0.55686277,  0.91372555,  0.98823535,  0.99215692,  0.98823535,\n",
       "        0.99215692,  0.98823535,  0.87450987,  0.07843138,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.48235297,  0.99607849,  0.99215692,  0.99607849,\n",
       "        0.99215692,  0.87843144,  0.7960785 ,  0.7960785 ,  0.87450987,\n",
       "        1.        ,  0.83529419,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.7960785 ,\n",
       "        0.99215692,  0.98823535,  0.99215692,  0.83137262,  0.07843138,\n",
       "        0.        ,  0.        ,  0.2392157 ,  0.99215692,  0.98823535,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.16078432,  0.95294124,  0.87843144,  0.7960785 ,\n",
       "        0.71764708,  0.16078432,  0.59607846,  0.11764707,  0.        ,\n",
       "        0.        ,  1.        ,  0.99215692,  0.40000004,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.15686275,  0.07843138,  0.        ,  0.        ,  0.40000004,\n",
       "        0.99215692,  0.19607845,  0.        ,  0.32156864,  0.99215692,\n",
       "        0.98823535,  0.07843138,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.32156864,  0.83921576,  0.12156864,\n",
       "        0.44313729,  0.91372555,  0.99607849,  0.91372555,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.24313727,  0.40000004,  0.32156864,\n",
       "        0.16078432,  0.99215692,  0.90980399,  0.99215692,  0.98823535,\n",
       "        0.91372555,  0.19607845,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.59607846,  0.99215692,  0.99607849,  0.99215692,  0.99607849,\n",
       "        0.99215692,  0.99607849,  0.91372555,  0.48235297,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.59607846,  0.98823535,\n",
       "        0.99215692,  0.98823535,  0.99215692,  0.98823535,  0.75294125,\n",
       "        0.19607845,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.24313727,  0.71764708,  0.7960785 ,  0.95294124,\n",
       "        0.99607849,  0.99215692,  0.24313727,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.15686275,  0.67450982,  0.98823535,\n",
       "        0.7960785 ,  0.07843138,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.08235294,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.71764708,  0.99607849,  0.43921572,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.24313727,\n",
       "        0.7960785 ,  0.63921571,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.2392157 ,  0.99215692,  0.59215689,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08235294,  0.83921576,  0.75294125,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04313726,  0.83529419,  0.99607849,\n",
       "        0.59215689,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.40000004,\n",
       "        0.99215692,  0.59215689,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.16078432,\n",
       "        0.83529419,  0.98823535,  0.99215692,  0.43529415,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.16078432,  1.        ,  0.83529419,\n",
       "        0.36078432,  0.20000002,  0.        ,  0.        ,  0.12156864,\n",
       "        0.36078432,  0.67843139,  0.99215692,  0.99607849,  0.99215692,\n",
       "        0.55686277,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.67450982,  0.98823535,  0.99215692,  0.98823535,\n",
       "        0.7960785 ,  0.7960785 ,  0.91372555,  0.98823535,  0.99215692,\n",
       "        0.98823535,  0.99215692,  0.50980395,  0.07843138,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.08235294,\n",
       "        0.7960785 ,  1.        ,  0.99215692,  0.99607849,  0.99215692,\n",
       "        0.99607849,  0.99215692,  0.95686281,  0.7960785 ,  0.32156864,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.07843138,  0.59215689,\n",
       "        0.59215689,  0.99215692,  0.67058825,  0.59215689,  0.59215689,\n",
       "        0.15686275,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "student"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABMAAAGKCAYAAADuTnrzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129390f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These indices are the examples you should show from mnist.train.images\n",
    "examples_to_show = np.array([0,  5100, 10200, 15300, 20400, 25500, 30600, 35700, 40800, 45900])\n",
    "\n",
    "# Get the examples from the training data\n",
    "examples = mnist.train.images\n",
    "\n",
    "# Convert each example into an image\n",
    "images =[example_to_image(examples[i]) for i in np.arange(55000)]\n",
    "\n",
    "# Call show_images using ncols=5\n",
    "show_images(examples, ncols=5, figsize=(10, 7)) # Hint: `use show_image`\n",
    "\n",
    "# We'll print the labels for each of these examples\n",
    "mnist.train.labels[examples_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "test",
     "q01"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-00bbe1d5405e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ok' is not defined"
     ]
    }
   ],
   "source": [
    "_ = ok.grade('q01')\n",
    "_ = ok.backup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are more than 2 labels (0 through 9), and the label data are represented in a one-hot encoding. So, the labels have dimension n x 10.  This is different from what we've done before, but it is is a typical strategy for *multiclass* classification.  We will see how our *softmax* loss function incorporates 10-dimensional labels.\n",
    "\n",
    "---\n",
    "<br></br>\n",
    "## Softmax Regression\n",
    "\n",
    "We've discussed logistic regression at length during lecture. The basic idea is that instead of taking the standard regression equation:\n",
    "\n",
    "$$ f_\\theta(x) = \\theta_1x_1 + ... + \\theta_dx_d + b = \\theta^\\top x + b $$\n",
    "\n",
    "We fit the sigmoid function instead:\n",
    "\n",
    "$$ f_\\theta(x) = s(\\theta_1x_1 + ... + \\theta_dx_d + b) = s(\\theta^\\top x + b) $$\n",
    "\n",
    "Where $$ s(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "The output of $s$ is always a number between 0 and 1, so we can roughly say, \"This example has a 70% chance of being in class 1 and 30% chance of being in class 2, so we'll label it class 1.\"\n",
    "\n",
    "When we have more than one class (say $J$ classes), we instead use the **softmax** function:\n",
    "\n",
    "$$ \\text{softmax}(x)_i = \\frac{e ^ {x_i}}{\\sum_{j=1}^{J} e^{x_j}} $$\n",
    "\n",
    "Which basically means: \"For an example $x$, give each possible class a score, then make sure all the scores add to 1 so we can say this example has a 50% chance of being a 0, 10% of being a 1, 15% of being a 2, etc.\"\n",
    "\n",
    "Then our regression function becomes:\n",
    "\n",
    "$$ f_\\theta(x) = \\text{softmax}(\\theta^\\top x + b) $$\n",
    "\n",
    "It's important to notice that the output of $f_\\theta$ and the input to $\\text{softmax}$ are 10-dimensional.  Since we learn a different score for each class, we need a whole row of parameters for each class.  Think about what that says about the dimensions of $\\theta$ and $b$.\n",
    "\n",
    "--- \n",
    "<br></br>\n",
    "## TensorFlow\n",
    "\n",
    "Let's code this up in TensorFlow. It's easy to implement this after you learn the syntax.\n",
    "\n",
    "Once you learn the basic syntax, you can create much more complicated models in a similar way. TensorFlow also  allows you to use your computer's GPU (graphical processing unit) to train your model, significantly decreasing training time.\n",
    "\n",
    "We're not going to doing very complicated things in TensorFlow today. However, we'll point out where it gives us flexibility that `scikit-learn` doesn't.\n",
    "\n",
    "<br></br>\n",
    "TensorFlow operates on variables and relationships between them.  Defining, training, and using a model has a few steps:\n",
    "\n",
    "1. We define variables for every quantity involved in the modeling process.  Some examples: the input to a model, the parameters of the model, any intermediate calculations done by the model, the outputs of the model, and the true labels we want to match.\n",
    "2. We describe the relationships between those variables; for example, multiplying the parameters by the inputs will produce our scores.\n",
    "3. We fill in the inputs and true labels, and we tell TensorFlow to use gradient descent to compute the best parameters.\n",
    "4. We can then fill in new inputs and observe the outputs of the trained model.\n",
    "\n",
    "\n",
    "<br></br>\n",
    "**Inputs**\n",
    "\n",
    "We use `tf.placeholder` to specify an input variable. In our case, we want our training data to be an input to the classifier (eg. training points in -> prediction out).\n",
    "\n",
    "The syntax is: `tf.placeholder( type , shape )` where `shape` is the shape of the input, like a NumPy array's shape.\n",
    "\n",
    "For example, `tf.placeholder(tf.int32, [50, 3])` says: \"This input takes in an integer array with 50 examples, 3 dimensions each.\"  Generally we don't hard-code the first dimension, the number of training examples, ourselves.  Instead, we write `tf.placeholder(tf.int32, [None, 3])`, which says: \"This input takes in an integer array with any number of examples, 3 dimensions each.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "# Question 2\n",
    "\n",
    "Create a placeholder called `x` that takes in a `tf.float32` array with any number of examples from the `mnist` dataset.\n",
    "\n",
    "Then, create a placeholder called `y_` that takes in a `tf.float32` array with any number of corresponding labels from the `mnist` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "x = ...\n",
    "y_ = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "# Question 3\n",
    "\n",
    "Weight and bias vectors are not determined by external input, but will be constantly updated while the gradient descent training process runs. The syntax to create such variables (initializing them to 0) is:\n",
    "\n",
    "`tf.Variable(tf.zeros( shape ))` where `shape` is the shape of the variable, again in NumPy style.\n",
    "\n",
    "Create variables `theta` and `b` corresponding to the weights and bias of our classifier.\n",
    "\n",
    "Remember that our prediction is a length 10 vector, *not* a single value as we have done before. This means that\n",
    "the dimensions of `theta` are *not* `(784, 1)` as usual. Think carefully about the dimensions of `x`, `theta`, `b`, and our prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "theta = ...\n",
    "b = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "# Question 4\n",
    "\n",
    "Now, we can implement our classifier.\n",
    "\n",
    "The `tf.nn.softmax(...)` function provides a softmax implementation for us. Instead of using the typical `X @ theta`, we use `tf.matmul(...)`.  Addition via `+` works as normal.\n",
    "\n",
    "Set `y` to the output of the softmax regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Loss Function \n",
    "\n",
    "`y` is a variable now.  Its value will be determined by the inputs `x` and parameters `theta` and `b`.\n",
    "\n",
    "We can implement all sorts of classifiers just by changing parts of the equation above. You just have to know the functional form of the classification function.\n",
    "\n",
    "In order to train our classifier, we need to implement the correct loss function. In class, we saw that the loss function for logistic regression was the negative log probability assigned by the model to the true labels. This translates directly to softmax regression. When there are multiple classes, it is called the *cross-entropy loss*:\n",
    "\n",
    "$$ L_{y}(\\hat{y}) = - \\sum_{j=1}^{J} y_j \\log \\hat{y}_j $$\n",
    "\n",
    "where $ y $ is the one-hot vector of the label and $ \\hat{y} $ is the vector of predicted softmax values.\n",
    "\n",
    "Verify that if we assign probability 1 to the correct label and 0 to the others, then the loss is 0.  It's also useful to verify that if the prediction is incorrect, the loss is greater than 0.\n",
    "\n",
    "Here's the cross entropy loss in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "We'll call `cross_entropy` the loss function, but as a Python object it's just another TensorFlow variable.  Its value is a scalar, the number we'd like to minimize by choosing `theta` and `b`.\n",
    "\n",
    "*Note:* Ordinarily you wouldn't have to write out the last two steps; TensorFlow provides [a single function](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification#softmax_cross_entropy_with_logits) that produces the cross-entropy loss given just $\\theta^T x + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "# Question 5 \n",
    "\n",
    "Now that we have written down our classification pipeline and loss, we need to tell TensorFlow how to run gradient descent.\n",
    "\n",
    "The syntax for this is:\n",
    "\n",
    "    tf.train.GradientDescentOptimizer( learning_rate ).minimize( loss_fn )\n",
    "\n",
    "Here `learning_rate` is the size of the steps we take at each iteration of gradient descent, and `loss_fn` is the variable defining the loss we'd like to minimize.\n",
    "\n",
    "Set `train_step` to the gradient descent rule using `0.5` as the learning rate and the cross entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "train_step = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br></br>\n",
    "\n",
    "## Train it!\n",
    "Our variables were containers or placeholders for data, with no numbers yet.  Similarly, `train_step` is a just a *recipe* for optimizing, embodied in a Python object.  We didn't actually do any optimization yet.  But we're ready now.\n",
    "\n",
    "The next cell tells TensorFlow to repeatedly compute `train_step`, filling in batches of 100 images at a time for `x` and `y_`.  This will update `theta` and `b` using stochastic gradient descent for 1000 iterations, using 100 examples per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "## How did we do?\n",
    "\n",
    "Run the next cell to see how your classifier did on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Test Accuracy:\")\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's see some examples of your predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLES_TO_SHOW = 10\n",
    "\n",
    "corrects = sess.run(correct_prediction, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "correct_i = np.where(corrects)[0][:EXAMPLES_TO_SHOW]\n",
    "\n",
    "print(\"Correct predictions:\")\n",
    "correct_ex = mnist.test.images[correct_i]\n",
    "correct_images = [example_to_image(example) for example in correct_ex]\n",
    "show_images(correct_images, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incorrect_i = np.where(~corrects)[0][:EXAMPLES_TO_SHOW]\n",
    "print(\"Incorrect predictions:\")\n",
    "incorrect_ex = mnist.test.images[incorrect_i]\n",
    "incorrect_images = [example_to_image(example) for example in incorrect_ex]\n",
    "show_images(incorrect_images, 5)\n",
    "\n",
    "print(\"You predicted:\")\n",
    "print(sess.run(tf.argmax(y,1), feed_dict={x: mnist.test.images, y_: mnist.test.labels})[incorrect_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "Chances are some of your incorrect predictions are hard for you to guess, too!\n",
    "\n",
    "We have only scratched the surface of TensorFlow.  If you'd like to continue, you can start at the [online tutorials](https://www.tensorflow.org/versions/r0.10/tutorials/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Run the cell below to run all the OkPy tests at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Running all tests...\")\n",
    "_ = ok.grade_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to submit your assignment to OkPy. The autograder should email you shortly with your autograded score. The autograder will only run once every 30 minutes.\n",
    "\n",
    "**If you're failing tests on the autograder but pass them locally**, you should simulate the autograder by doing the following:\n",
    "\n",
    "1. In the top menu, click Kernel -> Restart and Run all.\n",
    "2. Run the cell above to run each OkPy test.\n",
    "\n",
    "**You must make sure that you pass all the tests when running steps 1 and 2 in order.** If you are still failing autograder tests, you should double check your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
